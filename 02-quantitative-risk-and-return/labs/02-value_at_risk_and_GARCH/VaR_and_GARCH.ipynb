{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value at Risk\n",
    "There are several methods to evaluate risk for an individual stock or a portfolio, such as variance, standard deviation of returns, et al. But, those measures do not consider a probability distribution. However, many risk managers prefer a simple measure called Value at Risk (VaR). VaR is one of the most important metrics that is used to measure the risk associated with a financial position or a portfolio of financial instruments and can be defined as the maximum loss with a confidence level over a predetermined period.\n",
    "\n",
    "Let's say that the 1-day $95 \\%$ VaR of our portfolio is $\\$ 100$. This means that $95 \\%$ of the time, it is expected that - under normal market conditions - we will not lose more than $\\$ 100$ by holding our portfolio over one day.\n",
    "\n",
    "Three approaches that are commonly used in the industry are\n",
    "\n",
    "* Parametric\n",
    "\n",
    "* Historical\n",
    "\n",
    "* Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Data\n",
    "We'll get data from yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric VaR\n",
    "\n",
    "The Variance-covariance is a parametric method which assumes (almost\n",
    "always) that the returns are normally distributed. In this method, we\n",
    "first calculate the mean and standard deviation of the returns to derive\n",
    "the risk metric. Based on the assumption of normality, we can\n",
    "generalise, $V a R=$ position $*(\\mu-z * \\sigma)$\n",
    "\n",
    "<div class=\"center\">\n",
    "\n",
    "| Confidence Level | Value At Risk     |\n",
    "|:-----------------|:------------------|\n",
    "| 90%              | *μ* − 1.29 \\* *σ* |\n",
    "| 95%              | *μ* − 1.64 \\* *σ* |\n",
    "| 99%              | *μ* − 2.33 \\* *σ* |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality Test\n",
    "\n",
    "In the Parametric VaR, we assumed that the returns are normally\n",
    "distributed. However, in the real world, we know that stock / portfolio\n",
    "returns do not necessarily follow a normal distribution. Let’s perform a\n",
    "quick check to determine the normality of the underlying returns and see\n",
    "whether we need to modify our approach in deriving the VaR numbers.\n",
    "\n",
    "**Shapiro** The Shapiro-Wilk test is a test of normality and is used to\n",
    "determine whether or not a sample comes from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null hypothesis is that HDFCBank stock daily returns follows a normal distribution. Since the p-value is less than 0.05 , we reject the null hypothesis. We have sufficient evidence to say that the sample data does not come from a normal distribution. This result shouldn't be surprising as the data comes from an empirical distribution.\n",
    "\n",
    "**Anderson-Darling** Alternatively, we can perform an Anderson-Darling Test. It is a goodness of fit test that measures how well the data fit a specified distribution. This test is most commonly used to determine whether or not the data follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above result, the null hypothesis is rejected since the\n",
    "test statistic value is much higher than the critical value of 1.09 even\n",
    "at 1% significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified VaR\n",
    "\n",
    "Standard normal distribution have a zero mean, unit variance, zero\n",
    "skewness, and its kurtosis of 3. However, we now know the distribution\n",
    "is not normal and in such scenario, the skewness and excess kurtosis of\n",
    "many stock returns are not zero. As a consequence, the modified VaR was\n",
    "developed to utilize those four moments instead of the first two\n",
    "moments.\n",
    "\n",
    "$$\n",
    "m V a R=\\text { position } *(\\mu-t * \\sigma)\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "t=z+\\frac{1}{6}\\left(z^{2}-1\\right) s+\\frac{1}{24}\\left(z^{3}-3 z\\right) k-\\frac{1}{36}\\left(2 z^{3}-5 z\\right) s^{2}\n",
    "$$\n",
    "\n",
    "$\\mu$ is the return, $\\sigma$ is the volatility, $s$ is the skewness, $k$ is the kurtosis and $z$ is the absolute number of standard deviation from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical VaR\n",
    "\n",
    "Asset returns do not necessarily follow a normal distribution. An\n",
    "alternative is to use sorted returns to evaluate a VaR. This method uses\n",
    "historical data where returns are sorted in ascending order to calculate\n",
    "maximum possible loss for a given confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo VaR\n",
    "\n",
    "The Monte Carlo simulation approach has a number of similarities to\n",
    "historical simulation. It allows us to use actual historical\n",
    "distributions rather than having to assume normal returns. As returns\n",
    "are assumed to follow a normal distribution, we could generate *n*\n",
    "simulated returns with the same mean and standard deviation (derived\n",
    "from the daily returns) and then sorted in ascending order to calculate\n",
    "maximum possible loss for a given confidence level. \\[ \\]: \\# Set seed\n",
    "for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling VaR\n",
    "\n",
    "Now, let’s calculate VaR over a 5-day period. To scale it, multiply by\n",
    "square root of time.\n",
    "\n",
    "$$\n",
    "V a R=\\text { position } *(\\mu-z * \\sigma) * \\sqrt{T}\n",
    "$$\n",
    "\n",
    "where, $T$ is the horizon or forecast period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Short Fall\n",
    "\n",
    "$\\mathrm{VaR}$ is a reasonable measure of risk if assumption of normality holds. Else, we might underestimate the risk if we observe a fat tail or overestimate the risk if tail is thinner. Expected shortfall or Conditional Value at Risk - **CVaR** - is an estimate of expected shortfall sustained in the worst 1 - $x \\%$ of scenarios. It is defined as the average loss based on the returns that are lower than the VaR threshold. Assume that we have $n$ return observations, then the expected shortfall is\n",
    "\n",
    "$$\n",
    "C V a R=\\frac{1}{n} * \\sum_{i=1}^{n} R_{i}\\left[R \\leq h V a R_{c l}\\right]\n",
    "$$\n",
    "\n",
    "where, $R$ is returns, $h V a R$ is historical VaR and $c l$ is the confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio VaR\n",
    "If we know the returns and volatilities of all the assets in the portfolio, we can derive portfolio VaR. We will now derive VaR of minimum variance portfolio consisting of Indian stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GARCH\n",
    "Asset price volatility is central to derivatives pricing. It is defined as measure of price variability over certain period of time. In essence, it describes standard deviation of returns. There are different types of volatility: Historical, Implied, Forward. In most cases, we assume volatility to be constant, which is clearly not true and numerous studies have been dedicated to estimate this variable, both in academia and industry.\n",
    "\n",
    "## Volatility\n",
    "Volatility estimation by statistical means assume equal weights to all returns measured over the period. We know that over 1-day, the mean return is small as compared to standard deviation. If we consider a simple $m$-period moving average, where $\\sigma_{n}$ is the volatility of return on day $\\mathrm{n}$, then with $\\bar{u} \\approx 0$, we have\n",
    "\n",
    "$$\n",
    "\\sigma_{n}^{2}=\\frac{1}{m} \\sum_{i=1}^{m} u_{n-i}^{2}\n",
    "$$\n",
    "\n",
    "where, $u$ is return and $\\sigma^{2}$ is the variance.\n",
    "\n",
    "## ARCH\n",
    "However, any large return within this $n$ period will elevate the volatility until it drops out of the sample. Further, we observe volatility is mean reverting and tends to vary about a long term mean. To address this effect, we adopt to the weighting schemes.\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\sigma_{n}^{2}=\\gamma \\bar{\\sigma}^{2}+\\sum_{i=1}^{m} \\alpha_{i} u_{n-i}^{2} \\\\\n",
    "\\sigma_{n}^{2}=\\omega+\\sum_{i=1}^{m} \\alpha_{i} u_{n-i}^{2}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "where, $\\omega=\\gamma \\bar{\\sigma}^{2}$ and weights must sum to 1 .\n",
    "\n",
    "This is known as Autoregressive Conditional Heteroscedastic model. Autoregressive models are a statistical technique involving a regression of lagged values where the model suggests that past values can help forecast future values of the same variable. Within the model, a time series is the dependent variable and lagged values are the independent variables.\n",
    "\n",
    "The ARCH model, was originally developed by Robert Engle in 1982 to measure the dynamics of inflation uncertainty. Conditional heteroskedasticity refers to the notion that the next period's volatility is conditional on the volatility in the current period as well as to the time varying nature of volatility. However, given the volatility dynamics, this model fail to fully capture the persistence of volatility.\n",
    "\n",
    "## GARCH\n",
    "To address the shortcoming, ARCH has been extended to a generalised framework where we add volatility as a forecasting feature by adding previous variance. This method is popularly known as Generalized ARCH or GARCH model.\n",
    "\n",
    "$$\n",
    "\\sigma_{n}^{2}=\\omega+\\sum_{i=1}^{p} \\alpha_{i} u_{n-i}^{2}+\\sum_{i=1}^{q} \\beta_{i} \\sigma_{n-i}^{2}\n",
    "$$\n",
    "\n",
    "where, $p$ and $q$ are lag length.\n",
    "\n",
    "$\\operatorname{GARCH}(1,1)$ is then represented as,\n",
    "\n",
    "$$\n",
    "\\sigma_{n}^{2}=\\omega+\\alpha u_{n-1}^{2}+\\beta \\sigma_{n-1}^{2}\n",
    "$$\n",
    "\n",
    "where, $\\alpha+\\beta<1$ and $\\gamma+\\alpha+\\beta=1$ as weight applied to long term variance cannot be negative and $\\frac{\\omega}{(1-\\alpha-\\beta)}$ is the long-run variance.\n",
    "\n",
    "The GARCH model is a way of specifying the dependence of the time varying nature of volatility. The model incorporates changes in the fluctuations in volatility and tracks the persistence of volatility as it fluctuates around its long-term average and are exponentially weighted. To model GARCH or the conditional volatility, we need to derive $\\omega, \\alpha, \\beta$ by maximizing the likelihood function.\n",
    "\n",
    "## ARCH Toolbox\n",
    "$\\mathrm{ARCH}$ is one of the popular tools used for financial econometrics, written in Python - with Cython and/or Numba used to improve performance. We will now use arch\\_model to fit our GARCH model using this package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cqf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
